{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3341ac-53a3-43b0-afad-ed2c6099e107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcdc337-a064-4a9e-9a35-d27cd437b218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_silver_path=\"/Volumes/customer_360/customer_360_silver/silver_product_volume\"\n",
    "gold_path=\"/Volumes/customer_360/customer_360_gold/gold_product_dim_volume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1ad7ba-3bd4-41f2-b18f-3d4bc83b25e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"read silver data\").getOrCreate()\n",
    "df=spark.read\\\n",
    "    .format(\"delta\")\\\n",
    "    .load(src_silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d397bce-b545-46c0-b778-92ea0009a5b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, gold_path):\n",
    "    bronze_table = DeltaTable.forPath(spark, gold_path)\n",
    "    # Get max data_arrival_timestamp\n",
    "    max_ts_row = bronze_table.toDF().select(max(\"data_arrival_timestamp\")).collect()[0]\n",
    "    max_ts = max_ts_row[0]  # None if table is empty\n",
    "    if max_ts is None:\n",
    "        print(\"e table is empty. Will load all records.\")\n",
    "else:\n",
    "    print(\" table not found. Will load all records.\")\n",
    "    max_ts = None  # first load\n",
    "\n",
    "# Filter source for incremental load\n",
    "if max_ts:\n",
    "    df = df.filter(col(\"data_arrival_timestamp\") > max_ts)\n",
    "else:\n",
    "    df = df # first load, take all records\n",
    "\n",
    "print(f\"Number of records to load: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc0c2a0-cca7-4732-bec4-889908d85118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "# --- Step 1: Add tracking columns ---\n",
    "# Assuming df is your product DataFrame\n",
    "df = df.withColumn(\"start_date\", current_timestamp()) \\\n",
    "                       .withColumn(\"end_date\", lit(None).cast(TimestampType())) \\\n",
    "                       .withColumn(\"is_active\", ~col(\"is_deleted\"))  # active if not deleted\n",
    "\n",
    "df_written_product = None\n",
    "\n",
    "# --- Step 2: Check if Delta table exists ---\n",
    "if DeltaTable.isDeltaTable(spark, gold_path):\n",
    "    print(\"âœ… Product table exists. Applying SCD Type 2 logic...\")\n",
    "\n",
    "    delta_table = DeltaTable.forPath(spark, gold_path)\n",
    "    df_existing = delta_table.toDF()\n",
    "    df_active_existing = df_existing.filter(col(\"is_active\") == True)\n",
    "\n",
    "    # --- Step 3: Detect changed products ---\n",
    "    changes_df = df.alias(\"src\").join(\n",
    "        df_active_existing.alias(\"tgt\"),\n",
    "        on=col(\"src.product_id\") == col(\"tgt.product_id\"),\n",
    "        how=\"inner\"\n",
    "    ).filter(\n",
    "        (col(\"src.category\") != col(\"tgt.category\")) |\n",
    "        (col(\"src.product_name\") != col(\"tgt.product_name\")) |\n",
    "        (col(\"src.is_deleted\") != col(\"tgt.is_deleted\"))\n",
    "    ).select(\"src.product_id\").distinct()\n",
    "\n",
    "    # --- Step 4: Detect new products ---\n",
    "    df_new_products = df.alias(\"src\").join(\n",
    "        df_active_existing.alias(\"tgt\"),\n",
    "        on=col(\"src.product_id\") == col(\"tgt.product_id\"),\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Update changed products ---\n",
    "    if changes_df.count() > 0:\n",
    "        print(\"ðŸ” Found changed products â€” applying SCD Type 2 updates...\")\n",
    "        changed_ids = [r[\"product_id\"] for r in changes_df.collect()]\n",
    "\n",
    "        delta_table.update(\n",
    "            condition=col(\"product_id\").isin(changed_ids) & (col(\"is_active\") == True),\n",
    "            set={\n",
    "                \"is_active\": lit(False),\n",
    "                \"end_date\": current_timestamp()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df_changed_products = df.join(changes_df, on=\"product_id\", how=\"inner\")\n",
    "    else:\n",
    "        df_changed_products = spark.createDataFrame([], df.schema)\n",
    "\n",
    "    # --- Step 6: Combine new + changed products ---\n",
    "    df_to_insert = df_new_products.unionByName(df_changed_products)\n",
    "\n",
    "    # --- Step 7: Write if there are updates ---\n",
    "    if df_to_insert.count() > 0:\n",
    "        print(f\"ðŸ“¥ Appending {df_to_insert.count()} new/changed products...\")\n",
    "        df_written = spark.createDataFrame(df_to_insert.collect(), schema=df_to_insert.schema)\n",
    "        df_to_insert.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .partitionBy(\"category\") \\\n",
    "            .save(gold_path)\n",
    "        print(\"âœ… SCD Type 2 update completed successfully.\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No new or changed products. Nothing to insert.\")\n",
    "        df_written = spark.createDataFrame([], df.schema)\n",
    "\n",
    "else:\n",
    "    print(\"ðŸš€ Creating new Product Delta table (first load)...\")\n",
    "    df_written = spark.createDataFrame(df.collect(), schema=df.schema)\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .partitionBy(\"category\") \\\n",
    "        .save(gold_path)\n",
    "    print(\"âœ… New Product Delta table created.\")\n",
    "\n",
    "# df_written_product is for audit purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7101d6-b59b-48e9-9e46-505f1b040f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_written.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237a3bfc-21a7-4515-8d7f-aea8db1d3b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Count records\n",
    "records_count = df_written.count()\n",
    "\n",
    "if records_count==0:\n",
    "    msg=\" (Source Empty)\"\n",
    "else:\n",
    "    msg=\"\"\n",
    "\n",
    "# max timestamp (only if rows exist)\n",
    "max_data_ts_row = (\n",
    "    df_written.select(max(\"data_arrival_timestamp\")).collect()[0][0]\n",
    "    if records_count > 0\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Use Python datetime for load_time\n",
    "load_time = datetime.now()\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"layer\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"load_time\", TimestampType(), True),\n",
    "    StructField(\"records_loaded\", LongType(), True),\n",
    "    StructField(\"max_data_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Prepare audit data (even if 0 rows)\n",
    "data = [(\"gold\", f\"gold_product{msg}\", load_time, records_count, max_data_ts_row)]\n",
    "\n",
    "# Create DataFrame\n",
    "df_audit = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Append to audit table\n",
    "df_audit.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"/Volumes/customer_360/audit/audit_volume/etl_audit\")\n",
    "\n",
    "print(f\"Audit log updated successfully. Records loaded: {records_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f07baae-cabd-4451-a723-6211fac253e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta.`/Volumes/customer_360/customer_360_gold/gold_product_dim_volume`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0cfbe4-e6f4-4a91-83be-05aebe2d9607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TRUNCATE TABLE delta.`/Volumes/customer_360/customer_360_gold/gold_product_dim_volume`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2157eb72-f4e4-4ffc-b31c-5fea3cf39312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- UPDATE delta.`/Volumes/customer_360/customer_360_gold/gold_product_dim_volume`\n",
    "-- SET product_name = 'gg_18'\n",
    "-- WHERE product_id = 'PROD0001';\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6633152665297401,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver to gold product",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
