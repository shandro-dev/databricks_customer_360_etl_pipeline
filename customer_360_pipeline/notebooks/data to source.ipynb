{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf927f2-f8cd-458e-937e-9ea97d038c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1Ô∏è‚É£ Create Catalog\n",
    "CREATE CATALOG IF NOT EXISTS customer_360;\n",
    "\n",
    "-- 2Ô∏è‚É£ Create Schemas for each layer\n",
    "CREATE SCHEMA IF NOT EXISTS customer_360.customer_360_source;\n",
    "CREATE SCHEMA IF NOT EXISTS customer_360.customer_360_bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS customer_360.customer_360_silver;\n",
    "CREATE SCHEMA IF NOT EXISTS customer_360.customer_360_gold;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS customer_360.audit;\n",
    "\n",
    "-- 3Ô∏è‚É£ Create \"Volumes\" for each table (each volume corresponds to one table path)\n",
    "-- Source Layer Volumes\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_source.source_product_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_source.source_customer_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_source.source_sales_volume;\n",
    "\n",
    "-- Bronze Layer Volumes\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_bronze.bronze_product_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_bronze.bronze_customer_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_bronze.bronze_sales_volume;\n",
    "\n",
    "-- Silver Layer Volumes\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_silver.silver_product_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_silver.silver_customer_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_silver.silver_sales_volume;\n",
    "\n",
    "-- Gold Layer Volumes\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_gold.gold_sales_fact_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_gold.gold_product_dim_volume;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.customer_360_gold.gold_customer_dim_volume;\n",
    "\n",
    "-- audit\n",
    "CREATE SCHEMA IF NOT EXISTS customer_360.audit;\n",
    "CREATE VOLUME IF NOT EXISTS customer_360.audit.audit_volume;\n",
    "-- Create the table inside the volume\n",
    "CREATE TABLE IF NOT EXISTS delta.`/Volumes/customer_360/audit/audit_volume/etl_audit` (\n",
    "    layer STRING,                  -- bronze / silver / gold\n",
    "    table_name STRING,             -- e.g., bronze_customer\n",
    "    load_time TIMESTAMP,           -- ETL run timestamp\n",
    "    records_loaded BIGINT,         -- number of rows loaded in this run\n",
    "    max_data_timestamp TIMESTAMP   -- max data_arrival_timestamp of rows loaded\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "\n",
    "\n",
    "DROP SCHEMA IF EXISTS customer_360.default CASCADE;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b47f921-8c13-4a99-9ac8-53bcdb79c219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "fake = Faker()\n",
    "CURRENT_TS = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "DATA_ARRIVAL_TS = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# -------------------------\n",
    "# Helper Functions\n",
    "# -------------------------\n",
    "def rare_count(min_val, max_val, mostly_zero=True):\n",
    "    \"\"\"\n",
    "    Generate random numbers with bias towards zero (or small numbers)\n",
    "    mostly_zero=True: 80% chance to be 0\n",
    "    \"\"\"\n",
    "    if mostly_zero:\n",
    "        return random.choices(\n",
    "            range(min_val, max_val+1), \n",
    "            weights=[0.8 if i==0 else 0.2/(max_val-min_val) for i in range(min_val, max_val+1)]\n",
    "        )[0]\n",
    "    else:\n",
    "        return random.randint(min_val, max_val)\n",
    "\n",
    "def load_existing_csv(path):\n",
    "    import glob, os\n",
    "    files = glob.glob(f\"{path}/*.csv\")\n",
    "    if files:\n",
    "        df_list = [pd.read_csv(f) for f in files]\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        df = df.drop_duplicates(subset=[df.columns[0]])  # Remove duplicates by ID\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def generate_ids(prefix, n, existing_ids):\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    start_id = 1\n",
    "    if existing_ids:\n",
    "        nums = [int(i.replace(prefix,\"\")) for i in existing_ids if i.startswith(prefix)]\n",
    "        if nums:\n",
    "            start_id = max(nums) + 1\n",
    "    return [f\"{prefix}{str(i).zfill(4)}\" for i in range(start_id, start_id+n)]\n",
    "\n",
    "def save_unique_csv(df_new, df_existing, path, file_prefix):\n",
    "    if df_new.empty:\n",
    "        print(f\"‚òëÔ∏è No new rows to save for {file_prefix}\")\n",
    "        return\n",
    "    if not df_existing.empty:\n",
    "        existing_ids = set(df_existing[df_existing.columns[0]].tolist())\n",
    "        df_new = df_new[~df_new[df_new.columns[0]].isin(existing_ids)]\n",
    "    if not df_new.empty:\n",
    "        file_path = f\"{path}/{file_prefix}_{CURRENT_TS}.csv\"\n",
    "        df_new.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Saved {len(df_new)} new rows to {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚òëÔ∏è No new rows to save for {file_prefix}\")\n",
    "\n",
    "# -------------------------\n",
    "# Configurable Random Counts\n",
    "# -------------------------\n",
    "NUM_CUSTOMERS = rare_count(0, 4, mostly_zero=True)  # mostly 0 or 1\n",
    "NUM_PRODUCTS = rare_count(0, 2, mostly_zero=True)   # mostly 0\n",
    "NUM_SALES = rare_count(0, 21, mostly_zero=False)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# IMPORTANT:Uncomment the below 3 lines for initial load. Comment out after initial load\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# NUM_CUSTOMERS = 3 \n",
    "# NUM_PRODUCTS = 2   \n",
    "# NUM_SALES = 10\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "CUSTOMER_VOLUME = \"/Volumes/customer_360/customer_360_source/source_customer_volume\"\n",
    "PRODUCT_VOLUME = \"/Volumes/customer_360/customer_360_source/source_product_volume\"\n",
    "SALES_VOLUME = \"/Volumes/customer_360/customer_360_source/source_sales_volume\"\n",
    "\n",
    "# -------------------------\n",
    "# CUSTOMER\n",
    "# -------------------------\n",
    "df_customer_existing = load_existing_csv(CUSTOMER_VOLUME)\n",
    "existing_cust_ids = df_customer_existing[\"customer_id\"].tolist() if not df_customer_existing.empty else []\n",
    "customer_ids = generate_ids(\"CUST\", NUM_CUSTOMERS, existing_cust_ids)\n",
    "\n",
    "customer_data = []\n",
    "for cid in customer_ids:\n",
    "    customer_data.append({\n",
    "        \"customer_id\": cid,\n",
    "        \"customer_name\": fake.name(),\n",
    "        \"segment\": random.choice([\"Consumer\",\"Corporate\",\"Home Office\"]),\n",
    "        \"age\": random.randint(18,70),\n",
    "        \"country\": fake.country(),\n",
    "        \"city\": fake.city(),\n",
    "        \"state\": fake.state(),\n",
    "        \"postal_code\": fake.postcode(),\n",
    "        \"region\": random.choice([\"East\",\"West\",\"South\",\"North\"]),\n",
    "        \"data_arrival_timestamp\": DATA_ARRIVAL_TS\n",
    "    })\n",
    "\n",
    "df_customer_new = pd.DataFrame(customer_data)\n",
    "df_customer = pd.concat([df_customer_existing, df_customer_new], ignore_index=True) if not df_customer_existing.empty else df_customer_new\n",
    "df_customer = df_customer.drop_duplicates(subset=[\"customer_id\"])\n",
    "save_unique_csv(df_customer_new, df_customer_existing, CUSTOMER_VOLUME, \"customer\")\n",
    "\n",
    "# -------------------------\n",
    "# PRODUCT\n",
    "# -------------------------\n",
    "df_product_existing = load_existing_csv(PRODUCT_VOLUME)\n",
    "existing_prod_ids = df_product_existing[\"product_id\"].tolist() if not df_product_existing.empty else []\n",
    "product_ids = generate_ids(\"PROD\", NUM_PRODUCTS, existing_prod_ids)\n",
    "\n",
    "categories = {\n",
    "    \"Technology\": [\"Laptop\",\"Phone\",\"Tablet\",\"Camera\",\"Speaker\",\"Monitor\",\"Smartwatch\",\"Printer\",\"Router\",\"Headphones\",\"Projector\",\"Drone\",\"Keyboard\",\"Mouse\"],\n",
    "    \"Furniture\": [\"Chair\",\"Table\",\"Desk\",\"Shelf\",\"Sofa\",\"Cabinet\",\"Bed\",\"Stool\",\"Couch\",\"Wardrobe\",\"Nightstand\",\"Bench\"],\n",
    "    \"Office Supplies\": [\"Pen\",\"Notebook\",\"Binder\",\"Stapler\",\"Paper\",\"Envelope\",\"Marker\",\"Folder\",\"Tape\",\"Calculator\",\"Highlighter\",\"Clipboard\"],\n",
    "    \"Sports & Outdoors\": [\"Bicycle\",\"Treadmill\",\"Dumbbell\",\"Tent\",\"Backpack\",\"Yoga Mat\",\"Helmet\",\"Running Shoes\",\"Kayak\",\"Golf Club\",\"Hiking Boots\",\"Soccer Ball\"]\n",
    "}\n",
    "\n",
    "adjectives = [\"Ultra\",\"Pro\",\"Max\",\"Mini\",\"Smart\",\"Eco\",\"Advanced\",\"Premium\",\"Deluxe\",\"Compact\",\"Portable\",\"Elite\",\"I-\"]\n",
    "\n",
    "def generate_product_name_with_category(category):\n",
    "    noun = random.choice(categories[category])\n",
    "    adj = random.choice(adjectives)\n",
    "    number = random.randint(10,99)\n",
    "    return f\"{adj} {noun} {number}\"\n",
    "\n",
    "product_data = []\n",
    "for pid in product_ids:\n",
    "    category = random.choice(list(categories.keys()))\n",
    "    product_data.append({\n",
    "        \"product_id\": pid,\n",
    "        \"category\": category,\n",
    "        \"product_name\": generate_product_name_with_category(category),\n",
    "        \"data_arrival_timestamp\": DATA_ARRIVAL_TS\n",
    "    })\n",
    "\n",
    "df_product_new = pd.DataFrame(product_data)\n",
    "df_product = pd.concat([df_product_existing, df_product_new], ignore_index=True) if not df_product_existing.empty else df_product_new\n",
    "df_product = df_product.drop_duplicates(subset=[\"product_id\"])\n",
    "save_unique_csv(df_product_new, df_product_existing, PRODUCT_VOLUME, \"product\")\n",
    "\n",
    "# -------------------------\n",
    "# SALES\n",
    "# -------------------------\n",
    "df_sales_existing = load_existing_csv(SALES_VOLUME)\n",
    "existing_sales_ids = df_sales_existing[\"order_id\"].tolist() if not df_sales_existing.empty else []\n",
    "sales_ids = generate_ids(\"ORDER\", NUM_SALES, existing_sales_ids)\n",
    "\n",
    "available_customers = df_customer[\"customer_id\"].tolist() if not df_customer.empty else []\n",
    "available_products = df_product[\"product_id\"].tolist() if not df_product.empty else []\n",
    "\n",
    "if not available_customers or not available_products:\n",
    "    print(\"‚ö†Ô∏è Customers or products not available. Skipping sales generation.\")\n",
    "    sales_data = []\n",
    "else:\n",
    "    sales_data = []\n",
    "    for oid in sales_ids:\n",
    "        order_date = fake.date_between(start_date='-1y', end_date='today')\n",
    "        # Ship date 1‚Äì10 days after order_date\n",
    "        ship_date = order_date + timedelta(days=random.randint(1, 10))\n",
    "        sales_data.append({\n",
    "            \"order_id\": oid,\n",
    "            \"order_date\": order_date,\n",
    "            \"ship_date\": ship_date,\n",
    "            \"ship_mode\": random.choice([\"Standard Class\",\"Second Class\",\"First Class\",\"Same Day\"]),\n",
    "            \"customer_id\": random.choice(available_customers),\n",
    "            \"product_id\": random.choice(available_products),\n",
    "            \"sales\": round(random.uniform(20,1000),2),\n",
    "            \"quantity\": random.randint(1,5),\n",
    "            \"discount\": round(random.uniform(0,0.5),2),\n",
    "            \"profit\": round(random.uniform(5,200),2),\n",
    "            \"data_arrival_timestamp\": DATA_ARRIVAL_TS\n",
    "        })\n",
    "\n",
    "df_sales_new = pd.DataFrame(sales_data)\n",
    "df_sales = pd.concat([df_sales_existing, df_sales_new], ignore_index=True) if not df_sales_existing.empty else df_sales_new\n",
    "df_sales = df_sales.drop_duplicates(subset=[\"order_id\"])\n",
    "save_unique_csv(df_sales_new, df_sales_existing, SALES_VOLUME, \"sales\")\n",
    "\n",
    "print(\"üü¢ Data generation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e75ebab-9012-4a64-accc-e602acffe35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Folder path\n",
    "customer_folder = \"/Volumes/customer_360/customer_360_source/source_customer_volume\"\n",
    "product_folder = \"/Volumes/customer_360/customer_360_source/source_product_volume\"\n",
    "sales_folder = \"/Volumes/customer_360/customer_360_source/source_sales_volume\"\n",
    "\n",
    "# Read all CSVs in folder\n",
    "df_customers = spark.read.option(\"header\", True).csv(customer_folder + \"/*.csv\")\n",
    "df_products = spark.read.option(\"header\", True).csv(product_folder + \"/*.csv\")\n",
    "df_sales = spark.read.option(\"header\", True).csv(sales_folder + \"/*.csv\")\n",
    "\n",
    "# Show sample\n",
    "# df_customers.display(5)\n",
    "# df_products.display(5)\n",
    "# df_sales.display(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8887578453026614,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data to source",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
